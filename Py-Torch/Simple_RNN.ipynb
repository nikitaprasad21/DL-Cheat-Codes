{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7wzG0gH0659"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim # Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`nn.RNN` is a class within the PyTorch framework, specifically part of the torch.nn module. It is used to create an instance of a `recurrent neural network (RNN)` layer."
      ],
      "metadata": {
        "id": "WfK5SxDkBNW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Key Parameters of nn.RNN\n",
        "`input_size:`\n",
        "The number of expected features in the input x.\n",
        "\n",
        "`hidden_size:`\n",
        "The number of features in the hidden state h.\n",
        "\n",
        "`num_layers (optional):`\n",
        " Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results.\n",
        "\n",
        "`nonlinearity (optional):`\n",
        " The non-linearity to use. Can be either 'tanh' or 'relu'. Default is 'tanh'.\n",
        "\n",
        "`bias (optional):`\n",
        " If False, then the layer does not use bias weights b_ih and b_hh. Default is True.\n",
        "\n",
        "`batch_first (optional):`\n",
        " If True, then the input and output tensors are provided as (batch, seq, feature). Default is False, which expects (seq, batch, feature).\n",
        "\n",
        " `dropout (optional):`\n",
        " If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default is 0.\n",
        "\n",
        "`bidirectional (optional):`\n",
        " If True, becomes a bidirectional RNN. Default is False."
      ],
      "metadata": {
        "id": "qosuoTCDBap6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN model with Embedding\n",
        "class RNN1(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super (RNN1, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)  # RNN layer\n",
        "        self.fc = nn.Linear(hidden_size, output_size)  # Fully connected layer to produce the output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embed input words\n",
        "        x = self.embedding(x)\n",
        "        # Initialize hidden state with zeros (h0 i.e. 0th hidden state, it can be random numbers, zeros or ones)\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        # Forward propagate the RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        # Pass the output of the last time step to the fully connected layer\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10  # Size of the vocabulary (max integer index + 1)\n",
        "embedding_dim = 4  # Dimension of the embedding vectors\n",
        "hidden_size = 10  # Number of features in the hidden state\n",
        "output_size = 1  # Number of output classes\n",
        "\n",
        "# Create the model\n",
        "model = RNN1(vocab_size, embedding_dim, hidden_size, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Sample data (batch size, sequence length)\n",
        "inputs = torch.tensor([[1, 2, 3], [2, 3, 4]])\n",
        "targets = torch.tensor([[4.0], [5.0]])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "test_input = torch.tensor([[3, 4, 5]])\n",
        "predicted = model(test_input)\n",
        "print(f'Predicted value: {predicted.detach().numpy()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBI6s85X2VML",
        "outputId": "9aebb051-c2ca-4fb6-fc51-696661d1dce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 10.7145\n",
            "Epoch [20/100], Loss: 3.3133\n",
            "Epoch [30/100], Loss: 0.6507\n",
            "Epoch [40/100], Loss: 0.2449\n",
            "Epoch [50/100], Loss: 0.3256\n",
            "Epoch [60/100], Loss: 0.2804\n",
            "Epoch [70/100], Loss: 0.2404\n",
            "Epoch [80/100], Loss: 0.2399\n",
            "Epoch [90/100], Loss: 0.2370\n",
            "Epoch [100/100], Loss: 0.2316\n",
            "Predicted value: [[4.2711134]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Where we want to get the output at every time step.\n",
        "class RNN2(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        super(RNN2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        # Apply the fully connected layer to all time steps\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10  # Size of the vocabulary\n",
        "embedding_dim = 4  # Dimension of the embedding vectors\n",
        "hidden_size = 10  # Number of features in the hidden state\n",
        "output_size = 1  # Number of output classes per timestep\n",
        "\n",
        "# Create the model\n",
        "model = RNN2(vocab_size, embedding_dim, hidden_size, output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Sample data (batch size, sequence length)\n",
        "inputs = torch.tensor([[1, 2, 3], [2, 3, 4]])\n",
        "targets = torch.tensor([[[4.0], [5.0], [6.0]], [[5.0], [6.0], [7.0]]])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "test_input = torch.tensor([[3, 4, 5]])\n",
        "predicted = model(test_input)\n",
        "print(f'Predicted values: {predicted.detach().numpy()}')\n"
      ],
      "metadata": {
        "id": "A5MYZgE32V6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " `nn.Embedding` layer maps each integer in the input sequence to a high-dimensional vector. This layer is particularly useful when dealing with words where each word is represented as a unique integer.\n",
        "\n",
        "`nn.Embedding` layer transforms each integer in the input tensor into an embedding vector. The output shape from the embedding layer becomes (batch_size, sequence_length, embedding_dim).\n",
        "\n",
        " If embedding_dim is 4, as in the example, the shape after the embedding layer will be (2, 3, 4).\n",
        "\n",
        "`RNN Layer` When this tensor is passed through the nn.RNN layer, the RNN processes each sequence of embedded vectors. The nn.RNN layer outputs two tensors: the output tensor and the hidden state. The output tensor from the RNN has the general shape `(batch_size, sequence_length, num_directions * hidden_size)`, and `the hidden state` has the shape `(num_layers * num_directions, batch_size, hidden_size)`.\n",
        "\n",
        "`Fully Connected Layer` the model can uses the output at the last step or at every timestep of the sequence to make a prediction.\n",
        "\n",
        "If we want the prediction at the last time step output is sliced from the RNN output tensor with `out[:, -1, :]`, which reduces its shape to `(batch_size, hidden_size)`, or (2, 10).the sliced output is then passed through a fully connected layer `(nn.Linear)`, which is designed to map the RNN's hidden state to the desired output size.\n",
        "\n",
        "\n",
        "If we want the prediction at every time step output no need to slice the RNN output tensor simply pass it to `(nn.Linear)`, which is designed to map the RNN's hidden state to the desired output size.\n"
      ],
      "metadata": {
        "id": "WFuMaMgG8A55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "input_size = 3\n",
        "hidden_size = 4  # Each direction has 4 features\n",
        "num_layers = 1\n",
        "batch_size = 2\n",
        "seq_length = 5\n",
        "\n",
        "# Create a bidirectional RNN\n",
        "rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers,\n",
        "             bidirectional=True, batch_first=True)\n",
        "\n",
        "# Example input (batch size, sequence length, input size)\n",
        "input_tensor = torch.randn(batch_size, seq_length, input_size)\n",
        "\n",
        "# Forward pass\n",
        "out, hn = rnn(input_tensor)\n",
        "\n",
        "print(\"Output shape (out):\", out.shape)  # Expect [batch_size, seq_length, 2 * hidden_size]\n",
        "print(\"Last hidden state shape (hn):\", hn.shape)  # Expect [2 * num_layers, batch_size, hidden_size]\n",
        "\n",
        "# Output for first batch, first timestep\n",
        "print(\"Forward pass output (first half):\", out[0, 0, :hidden_size])\n",
        "print(\"Backward pass output (second half):\", out[0, 0, hidden_size:])\n"
      ],
      "metadata": {
        "id": "bLpfakLu6zH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89adf15d-cc4b-45be-a321-ecde0b3a006d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape (out): torch.Size([2, 5, 8])\n",
            "Last hidden state shape (hn): torch.Size([2, 2, 4])\n",
            "Forward pass output (first half): tensor([-0.4961, -0.0098,  0.1262,  0.6290], grad_fn=<SliceBackward0>)\n",
            "Backward pass output (second half): tensor([ 0.0261, -0.3007, -0.6989,  0.2977], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bidirectional RNN"
      ],
      "metadata": {
        "id": "bYY0ytb2vzTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "input_size = 3\n",
        "hidden_size = 4\n",
        "num_layers = 2  # Two layers of RNN\n",
        "batch_size = 1\n",
        "seq_length = 5\n",
        "bidirectional = True\n",
        "\n",
        "# Define a multi-layer bidirectional RNN\n",
        "rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers,\n",
        "             bidirectional=bidirectional, batch_first=True)\n",
        "\n",
        "# Example input tensor\n",
        "input_tensor = torch.randn(batch_size, seq_length, input_size)\n",
        "\n",
        "# Forward pass through the RNN\n",
        "out, hn = rnn(input_tensor)\n",
        "\n",
        "print(\"Output shape (out):\", out.shape)  # Expect [batch_size, seq_length, 2 * hidden_size]\n",
        "print(\"Hidden state shape (hn):\", hn.shape)  # Expect [num_layers * num_directions, batch_size, hidden_size]\n",
        "\n",
        "# Accessing and using the hidden states\n",
        "hn_forward_layer1 = hn[0, :, :]  # First layer, forward direction\n",
        "hn_backward_layer1 = hn[1, :, :]  # First layer, backward direction\n",
        "hn_forward_layer2 = hn[2, :, :]  # Second layer, forward direction\n",
        "hn_backward_layer2 = hn[3, :, :]  # Second layer, backward direction\n",
        "\n",
        "# Example of concatenating forward and backward hidden states for each layer\n",
        "concatenated_layer1 = torch.cat((hn_forward_layer1, hn_backward_layer1), dim=1)\n",
        "concatenated_layer2 = torch.cat((hn_forward_layer2, hn_backward_layer2), dim=1)\n",
        "\n",
        "# Output for verification\n",
        "print(\"Concatenated hidden states, layer 1:\", concatenated_layer1.shape)\n",
        "print(\"Concatenated hidden states, layer 2:\", concatenated_layer2.shape)"
      ],
      "metadata": {
        "id": "owcgaA5J7S1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep BiRNN"
      ],
      "metadata": {
        "id": "Qaaee2QIv6_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepBiRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers, output_all_timesteps=False):\n",
        "        super(DeepBiRNN, self).__init__()\n",
        "        self.output_all_timesteps = output_all_timesteps\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                          batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)  # *2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)  # No need to initialize h0 explicitly; it defaults to zero\n",
        "\n",
        "        if self.output_all_timesteps:\n",
        "            # Apply the fully connected layer to all timesteps\n",
        "            out = self.fc(out)\n",
        "        else:\n",
        "            # Apply the fully connected layer only to the final timestep's output\n",
        "            out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out\n",
        "\n",
        "# Parameters for the model\n",
        "vocab_size = 100\n",
        "embedding_dim = 50\n",
        "hidden_size = 20\n",
        "output_size = 1\n",
        "num_layers = 2  # More than one layer makes it a deep RNN\n",
        "\n",
        "# Model instantiation for output at every timestep\n",
        "model_all_timesteps = DeepBiRNN(vocab_size, embedding_dim, hidden_size, output_size, num_layers, output_all_timesteps=True)\n",
        "\n",
        "# Model instantiation for output at only the last timestep\n",
        "model_last_timestep = DeepBiRNN(vocab_size, embedding_dim, hidden_size, output_size, num_layers, output_all_timesteps=False)\n",
        "\n",
        "# Example Input\n",
        "input_tensor = torch.tensor([[1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "\n",
        "# Forward pass for both models\n",
        "output_all_timesteps = model_all_timesteps(input_tensor)\n",
        "output_last_timestep = model_last_timestep(input_tensor)\n",
        "\n",
        "print(\"Output (All Timesteps):\", output_all_timesteps.shape)  # (batch_size, sequence_length, output_size)\n",
        "print(\"Output (Last Timestep):\", output_last_timestep.shape)  # (batch_size, output_size)\n"
      ],
      "metadata": {
        "id": "Uc1EDbUe5veb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5tLllHfeA7nN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}